{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28697972-61a5-4bbe-96dd-424887672e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''pip install yfinance'''\n",
    "'''pip install nbimporter'''\n",
    "'''pip install psycopg2'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a026ce6-6785-442b-befb-01376af7a08c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8b8ab0c-8b0a-40d7-afed-50482fa48462",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install nbimporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faff195f-b88b-4cc7-935c-adea5e1d1a8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aad4fcea-3bb0-490c-882c-4d6c3e123aa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import configparser\n",
    "import datetime\n",
    "import nbimporter\n",
    "from General import *\n",
    "from Daily_etl import *\n",
    "from SQL import *\n",
    "from Functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b46a6ef1-ae99-47a0-9789-9ed6c4b89417",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the database successfully.\n"
     ]
    }
   ],
   "source": [
    "#connecting to the database\n",
    "\n",
    "\n",
    "def connect_to_database_AWS_postgreSQL():\n",
    "    try:\n",
    "        # Read database credentials from the configuration file\n",
    "        config = configparser.ConfigParser()\n",
    "        config.read('configuration.ini')\n",
    "\n",
    "        host = config['database']['host']\n",
    "        port = int(config['database']['port'])\n",
    "        user = config['database']['user']\n",
    "        password = config['database']['password']\n",
    "        database = config['database']['database']\n",
    "\n",
    "       # Establish a connection to the database using the credentials from the config file\n",
    "        connection = psycopg2.connect(host=host, port=port, user=user, password=password, database=database)\n",
    "\n",
    "        return connection\n",
    "\n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(f\"Error: {error}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Establish a connection to the database\n",
    "    connection = connect_to_database_AWS_postgreSQL()\n",
    "\n",
    "    if connection:\n",
    "        print(\"Connected to the database successfully.\")\n",
    "\n",
    "        connection.close()\n",
    "    else:\n",
    "        print(\"Failed to connect to the database.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c61f264-ccd8-4a07-94b2-050d95811ea4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the database successfully.\n",
      "Tables in the database:\n",
      "f_daily_facts\n",
      "ticker_and_name\n",
      "s_historical_stock_data\n",
      "f_dates_of_stock_info\n",
      "s_date\n",
      "d_dated\n",
      "predictive_model5_results_aws_postgresql\n",
      "s_customer\n",
      "predictive_model6_results_aws_postgresql\n",
      "di_financial_info\n",
      "d_customer\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#testing to see the number of tables in the database\n",
    "#********************************************************************************************************\n",
    "#********************************************************************************************************\n",
    "def connect_to_database_AWS_postgreSQL():\n",
    "    try:\n",
    "        # Read database credentials from the configuration file\n",
    "        config = configparser.ConfigParser()\n",
    "        config.read('configuration.ini')\n",
    "\n",
    "        host = config['database']['host']\n",
    "        port = int(config['database']['port'])\n",
    "        user = config['database']['user']\n",
    "        password = config['database']['password']\n",
    "        database = config['database']['database']\n",
    "\n",
    "       # Establish a connection to the database using the credentials from the config file\n",
    "        connection = psycopg2.connect(host=host, port=port, user=user, password=password, database=database)\n",
    "\n",
    "        return connection\n",
    "\n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(f\"Error: {error}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Establish a connection to the database\n",
    "    connection = connect_to_database_AWS_postgreSQL()\n",
    "\n",
    "    if connection:\n",
    "        print(\"Connected to the database successfully.\")\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # SQL query to get table names\n",
    "        query = \"\"\"\n",
    "            SELECT table_name\n",
    "            FROM information_schema.tables\n",
    "            WHERE table_schema = 'public'\n",
    "                AND table_type = 'BASE TABLE';\n",
    "        \"\"\"\n",
    "\n",
    "        # Execute the query\n",
    "        cursor.execute(query)\n",
    "\n",
    "        # Fetch all rows\n",
    "        tables = cursor.fetchall()\n",
    "\n",
    "        # Print the table names\n",
    "        print(\"Tables in the database:\")\n",
    "        for table in tables:\n",
    "            print(table[0])\n",
    "\n",
    "        # Close cursor and connection\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "    else:\n",
    "        print(\"Failed to connect to the database.\")\n",
    "        # Create a cursor object\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23cc8415-6d98-4006-8e42-c8144f8d3947",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to execute SQL queries\n",
    "#SQL REFERENCE A\n",
    "\n",
    "\n",
    "def execute_sql_query_AWS_postgreSQL(query, params=None):\n",
    "    \n",
    "    # Establish a connection to the database\n",
    "    connection = connect_to_database_AWS_postgreSQL()\n",
    "\n",
    "    try:\n",
    "        if connection:\n",
    "            cursor = connection.cursor()\n",
    "            \n",
    "            # Execute the SQL query\n",
    "            if params:\n",
    "                cursor.execute(query, params)\n",
    "            else:\n",
    "                cursor.execute(query)\n",
    "\n",
    "            # If it's a SELECT query, fetch the results\n",
    "            if query.strip().lower().startswith(\"select\"):\n",
    "                results = cursor.fetchall()\n",
    "                cursor.close()\n",
    "                return results\n",
    "\n",
    "            # For other types of queries, commit the changes\n",
    "            else:\n",
    "                connection.commit()\n",
    "               # print(\"Query executed successfully.\")\n",
    "\n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(f\"Error: {error}\")\n",
    "        if connection:\n",
    "            connection.rollback()\n",
    "\n",
    "    finally:\n",
    "        if connection:\n",
    "            connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f10a966-c18c-4430-a5ea-473edb6c6a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "##to see is the yfinance library works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ffc9f2b0-5d0f-4715-ac03-e713b9326263",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Open         High          Low        Close  \\\n",
      "Date                                                                            \n",
      "2019-05-02 00:00:00+01:00   273.756256   273.756256   273.756256   273.756256   \n",
      "2019-05-23 00:00:00+01:00   276.249390   276.249390   276.249390   276.249390   \n",
      "2019-06-17 00:00:00+01:00   277.191681   277.191681   277.191681   277.191681   \n",
      "2019-06-18 00:00:00+01:00   279.056641   279.056641   279.056641   279.056641   \n",
      "2019-06-19 00:00:00+01:00   277.682465   277.682465   277.682465   277.682465   \n",
      "...                                ...          ...          ...          ...   \n",
      "2024-04-12 00:00:00+01:00  7254.429199  7263.049805  7247.616699  7247.616699   \n",
      "2024-04-15 00:00:00+01:00  7260.049805  7260.049805  7253.982910  7253.982910   \n",
      "2024-04-17 00:00:00+01:00  7244.528809  7254.294922  7244.528809  7254.294922   \n",
      "2024-04-19 00:00:00+01:00  7262.946777  7267.740234  7250.259766  7257.000488   \n",
      "2024-04-23 00:00:00+01:00  7264.006836  7264.006836  7264.006836  7264.006836   \n",
      "\n",
      "                           Volume  Dividends  Stock Splits  Capital Gains  \n",
      "Date                                                                       \n",
      "2019-05-02 00:00:00+01:00    2128        0.0           0.0            0.0  \n",
      "2019-05-23 00:00:00+01:00     220        0.0           0.0            0.0  \n",
      "2019-06-17 00:00:00+01:00    6086        0.0           0.0            0.0  \n",
      "2019-06-18 00:00:00+01:00    1140        0.0           0.0            0.0  \n",
      "2019-06-19 00:00:00+01:00     178        0.0           0.0            0.0  \n",
      "...                           ...        ...           ...            ...  \n",
      "2024-04-12 00:00:00+01:00    1211        0.0           0.0            0.0  \n",
      "2024-04-15 00:00:00+01:00     387        0.0           0.0            0.0  \n",
      "2024-04-17 00:00:00+01:00    3899        0.0           0.0            0.0  \n",
      "2024-04-19 00:00:00+01:00     836        0.0           0.0            0.0  \n",
      "2024-04-23 00:00:00+01:00   23162        0.0           0.0            0.0  \n",
      "\n",
      "[613 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "#This is a sample test and will be used for testing only. \n",
    "#It utilises yfinance library. It takes \"Ticker\" symbol then returns the ticker information. \n",
    "#Fetch data for a single stock symbol\n",
    "#This is used to get financial information on stock market from yahoo finance using ticker as test \n",
    "\n",
    "stock = yf.Ticker(\"0A0B.L\")\n",
    "hist = stock.history(period=\"6y\")  # Fetches one month of historical data\n",
    "\n",
    "print(hist)\n",
    "#If you want to see the result just uncomment the print statement.\n",
    "#This was a test that was run to ensure that the code is able to work on one ticker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da469ead-b44b-4b79-b366-c84dba55a5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I know need to insert Symbol and description into the aws .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ef625ea-8cbf-4485-b272-2587513947cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#I now need to creat the table to store the sybol and description information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8972b9c9-70d5-48bf-82f7-33277205a577",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef create_tables():\\n    try:\\n        # SQL statements to create tables\\n        create_table_queries = [\\n           #To store the symbol and description.\\n            \"\"\"\\n            CREATE TABLE ticker_and_name (\\n            Symbol TEXT,\\n            Description TEXT\\n            )\\n            \"\"\"\\n            ,\\n            \\n            #creating s_date table to store date information.\\n            \"\"\"\\n            CREATE TABLE s_Date (\\n            DateKey SERIAL PRIMARY KEY,\\n            Date DATE NOT NULL,\\n            Year INT,\\n            Quarter INT,\\n            Month INT,\\n            Week VARCHAR(2),\\n            Day INT,\\n            DayOfWeek VARCHAR(9),\\n            IsWeekend BOOLEAN,\\n            IsHoliday BOOLEAN,\\n            FiscalYear INT,\\n            FiscalQuarter INT,\\n            FiscalMonth INT\\n            )\\n            \"\"\"\\n            ,\\n            \\n            #creating d_dated table to store date information.\\n            \"\"\"\\n            CREATE TABLE d_dated (\\n            Date DATE PRIMARY KEY,--this will go into facts\\n            Year INT,\\n            Quarter INT,\\n            Month INT,\\n            Week VARCHAR(5),\\n            Day INT,\\n            DayOfWeek VARCHAR(10),\\n            IsWeekend BOOLEAN,--this will go into facts\\n            IsHoliday BOOLEAN,\\n            FiscalYear INT,\\n            FiscalQuarter INT,\\n            FiscalMonth INT\\n            )\\n            \"\"\"\\n            , \\n            \\n            ##creating s_customer table to store cormporate information.\\n            \"\"\"\\n            CREATE TABLE s_customer (\\n            Address1 VARCHAR(255), \\n            City VARCHAR(255),\\n            State VARCHAR(2),\\n            Zip VARCHAR(10),\\n            Country VARCHAR(255),\\n            Phone VARCHAR(20),\\n            Website VARCHAR(255),\\n            Industry VARCHAR(255),\\n            Sector VARCHAR(255),\\n            SectorKey VARCHAR(255),\\n            symbol VARCHAR (255),\\n            customer_name TEXT,\\n            full_time_employees INT,\\n            profit_margins NUMERIC(25, 2),\\n            earnings_growth NUMERIC(25, 2),\\n            revenue_growth NUMERIC(25, 2),\\n            market_cap NUMERIC(25, 2),\\n            fifty_two_week_high NUMERIC(25, 2),\\n            fifty_two_week_low NUMERIC(25, 2),\\n            currency TEXT,\\n            exchange TEXT\\n            )\\n            \"\"\"\\n            ,\\n            \\n            ##creating d_customer table to store cormporate information.\\n            \"\"\"\\n            CREATE TABLE d_customer (\\n            Address VARCHAR(255),\\n            City VARCHAR(255),\\n            State VARCHAR(2),\\n            Zip VARCHAR(10),\\n            Country VARCHAR(255),\\n            Phone VARCHAR(20),\\n            Website VARCHAR(255),\\n            Industry VARCHAR(255),\\n            Sector VARCHAR(255),\\n            SectorKey VARCHAR(255),\\n            symbol VARCHAR (255),\\n            Company_name TEXT,\\n            full_time_employees INT,\\n            Profit_margins NUMERIC(25, 2),\\n            Earnings_growth NUMERIC(25, 2), \\n            Revenue_growth NUMERIC(25, 2),\\n            Market_cap NUMERIC(25, 2), \\n            Fifty_two_week_high NUMERIC(25, 2),\\n            Fifty_two_week_low NUMERIC(25, 2),\\n            Currency TEXT,\\n            Exchange TEXT, \\n            virtual VARCHAR(255)\\n            )\\n            \"\"\"\\n            , \\n            \\n            #CREATING TABLE TO STORE STOCK INFORMATION\\n            \"\"\"\\n            CREATE TABLE s_historical_stock_data (\\n            symbol TEXT,\\n            date DATE,\\n            open NUMERIC(25, 2),\\n            high NUMERIC(25, 2),\\n            low NUMERIC(25, 2),\\n            close NUMERIC(25, 2),\\n            volume INT,\\n            dividends NUMERIC(25, 2),\\n            stock_splits NUMERIC(25, 2)\\n            )\\n            \"\"\"\\n            , \\n            \\n            #Creating di_financial_info table to store stock information\\n            \\n            \"\"\"\\n            Create table di_financial_info(\\n            symbol TEXT,\\n            date DATE,\\n            open NUMERIC(25, 2),\\n            high NUMERIC(25, 2),\\n            low NUMERIC(25, 2),\\n            close NUMERIC(25, 2),\\n            volume INT,\\n            dividends NUMERIC(25, 2),\\n            stock_splits NUMERIC(25, 2)\\n            )\\n            \"\"\"\\n            , \\n            \\n            #Creating a facts table. Facts table holds information from each company. \\n            #facts table will be a big one as this will be loaded ideally on the daily bassis. \\n            \\n            \"\"\"\\n            create table f_daily_facts(\\n            Date DATE,\\n            IsWeekend BOOLEAN,\\n            symbol TEXT,\\n            open NUMERIC(25, 2),\\n            high NUMERIC(25, 2),\\n            low NUMERIC(25, 2),\\n            close NUMERIC(25, 2),\\n            volume INT,\\n            profit_margins NUMERIC(25, 2),\\n            earnings_growth NUMERIC(25, 2),\\n            revenue_growth NUMERIC(25, 2),\\n            sector Text\\n            )\\n            \"\"\"\\n            , \\n            #Creating table to store symbol and date from the fact table.\\n            #This will aid in ensuring the data is up to date. \\n            \\n            \"\"\"\\n            Create table f_dates_of_stock_info (\\n            Symbol text, \\n            lastdate date,\\n            todays_date date\\n            )\\n            \"\"\"            \\n        ]\\n\\n        # Execute the SQL statements to create tables\\n        for query in create_table_queries:\\n            execute_sql_query_AWS_postgreSQL(query)\\n\\n        print(\"Tables created successfully.\")\\n\\n    except (Exception, psycopg2.Error) as error:\\n        print(f\"Error: {error}\")\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To save time added in this code to create tables that are needed in one go. \n",
    "# this will include source, dimension and facts. \n",
    "'''\n",
    "def create_tables():\n",
    "    try:\n",
    "        # SQL statements to create tables\n",
    "        create_table_queries = [\n",
    "           #To store the symbol and description.\n",
    "            \"\"\"\n",
    "            CREATE TABLE ticker_and_name (\n",
    "            Symbol TEXT,\n",
    "            Description TEXT\n",
    "            )\n",
    "            \"\"\"\n",
    "            ,\n",
    "            \n",
    "            #creating s_date table to store date information.\n",
    "            \"\"\"\n",
    "            CREATE TABLE s_Date (\n",
    "            DateKey SERIAL PRIMARY KEY,\n",
    "            Date DATE NOT NULL,\n",
    "            Year INT,\n",
    "            Quarter INT,\n",
    "            Month INT,\n",
    "            Week VARCHAR(2),\n",
    "            Day INT,\n",
    "            DayOfWeek VARCHAR(9),\n",
    "            IsWeekend BOOLEAN,\n",
    "            IsHoliday BOOLEAN,\n",
    "            FiscalYear INT,\n",
    "            FiscalQuarter INT,\n",
    "            FiscalMonth INT\n",
    "            )\n",
    "            \"\"\"\n",
    "            ,\n",
    "            \n",
    "            #creating d_dated table to store date information.\n",
    "            \"\"\"\n",
    "            CREATE TABLE d_dated (\n",
    "            Date DATE PRIMARY KEY,--this will go into facts\n",
    "            Year INT,\n",
    "            Quarter INT,\n",
    "            Month INT,\n",
    "            Week VARCHAR(5),\n",
    "            Day INT,\n",
    "            DayOfWeek VARCHAR(10),\n",
    "            IsWeekend BOOLEAN,--this will go into facts\n",
    "            IsHoliday BOOLEAN,\n",
    "            FiscalYear INT,\n",
    "            FiscalQuarter INT,\n",
    "            FiscalMonth INT\n",
    "            )\n",
    "            \"\"\"\n",
    "            , \n",
    "            \n",
    "            ##creating s_customer table to store cormporate information.\n",
    "            \"\"\"\n",
    "            CREATE TABLE s_customer (\n",
    "            Address1 VARCHAR(255), \n",
    "            City VARCHAR(255),\n",
    "            State VARCHAR(2),\n",
    "            Zip VARCHAR(10),\n",
    "            Country VARCHAR(255),\n",
    "            Phone VARCHAR(20),\n",
    "            Website VARCHAR(255),\n",
    "            Industry VARCHAR(255),\n",
    "            Sector VARCHAR(255),\n",
    "            SectorKey VARCHAR(255),\n",
    "            symbol VARCHAR (255),\n",
    "            customer_name TEXT,\n",
    "            full_time_employees INT,\n",
    "            profit_margins NUMERIC(25, 2),\n",
    "            earnings_growth NUMERIC(25, 2),\n",
    "            revenue_growth NUMERIC(25, 2),\n",
    "            market_cap NUMERIC(25, 2),\n",
    "            fifty_two_week_high NUMERIC(25, 2),\n",
    "            fifty_two_week_low NUMERIC(25, 2),\n",
    "            currency TEXT,\n",
    "            exchange TEXT\n",
    "            )\n",
    "            \"\"\"\n",
    "            ,\n",
    "            \n",
    "            ##creating d_customer table to store cormporate information.\n",
    "            \"\"\"\n",
    "            CREATE TABLE d_customer (\n",
    "            Address VARCHAR(255),\n",
    "            City VARCHAR(255),\n",
    "            State VARCHAR(2),\n",
    "            Zip VARCHAR(10),\n",
    "            Country VARCHAR(255),\n",
    "            Phone VARCHAR(20),\n",
    "            Website VARCHAR(255),\n",
    "            Industry VARCHAR(255),\n",
    "            Sector VARCHAR(255),\n",
    "            SectorKey VARCHAR(255),\n",
    "            symbol VARCHAR (255),\n",
    "            Company_name TEXT,\n",
    "            full_time_employees INT,\n",
    "            Profit_margins NUMERIC(25, 2),\n",
    "            Earnings_growth NUMERIC(25, 2), \n",
    "            Revenue_growth NUMERIC(25, 2),\n",
    "            Market_cap NUMERIC(25, 2), \n",
    "            Fifty_two_week_high NUMERIC(25, 2),\n",
    "            Fifty_two_week_low NUMERIC(25, 2),\n",
    "            Currency TEXT,\n",
    "            Exchange TEXT, \n",
    "            virtual VARCHAR(255)\n",
    "            )\n",
    "            \"\"\"\n",
    "            , \n",
    "            \n",
    "            #CREATING TABLE TO STORE STOCK INFORMATION\n",
    "            \"\"\"\n",
    "            CREATE TABLE s_historical_stock_data (\n",
    "            symbol TEXT,\n",
    "            date DATE,\n",
    "            open NUMERIC(25, 2),\n",
    "            high NUMERIC(25, 2),\n",
    "            low NUMERIC(25, 2),\n",
    "            close NUMERIC(25, 2),\n",
    "            volume INT,\n",
    "            dividends NUMERIC(25, 2),\n",
    "            stock_splits NUMERIC(25, 2)\n",
    "            )\n",
    "            \"\"\"\n",
    "            , \n",
    "            \n",
    "            #Creating di_financial_info table to store stock information\n",
    "            \n",
    "            \"\"\"\n",
    "            Create table di_financial_info(\n",
    "            symbol TEXT,\n",
    "            date DATE,\n",
    "            open NUMERIC(25, 2),\n",
    "            high NUMERIC(25, 2),\n",
    "            low NUMERIC(25, 2),\n",
    "            close NUMERIC(25, 2),\n",
    "            volume INT,\n",
    "            dividends NUMERIC(25, 2),\n",
    "            stock_splits NUMERIC(25, 2)\n",
    "            )\n",
    "            \"\"\"\n",
    "            , \n",
    "            \n",
    "            #Creating a facts table. Facts table holds information from each company. \n",
    "            #facts table will be a big one as this will be loaded ideally on the daily bassis. \n",
    "            \n",
    "            \"\"\"\n",
    "            create table f_daily_facts(\n",
    "            Date DATE,\n",
    "            IsWeekend BOOLEAN,\n",
    "            symbol TEXT,\n",
    "            open NUMERIC(25, 2),\n",
    "            high NUMERIC(25, 2),\n",
    "            low NUMERIC(25, 2),\n",
    "            close NUMERIC(25, 2),\n",
    "            volume INT,\n",
    "            profit_margins NUMERIC(25, 2),\n",
    "            earnings_growth NUMERIC(25, 2),\n",
    "            revenue_growth NUMERIC(25, 2),\n",
    "            sector Text\n",
    "            )\n",
    "            \"\"\"\n",
    "            , \n",
    "            #Creating table to store symbol and date from the fact table.\n",
    "            #This will aid in ensuring the data is up to date. \n",
    "            \n",
    "            \"\"\"\n",
    "            Create table f_dates_of_stock_info (\n",
    "            Symbol text, \n",
    "            lastdate date,\n",
    "            todays_date date\n",
    "            )\n",
    "            \"\"\"            \n",
    "        ]\n",
    "\n",
    "        # Execute the SQL statements to create tables\n",
    "        for query in create_table_queries:\n",
    "            execute_sql_query_AWS_postgreSQL(query)\n",
    "\n",
    "        print(\"Tables created successfully.\")\n",
    "\n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(f\"Error: {error}\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "18108a14-623f-4b69-8d3b-aa5adec97337",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff172bdb-6654-4402-bd26-9debfe92684c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2ccc95-fe8e-4479-b558-e878a44400d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Populate the ticker and name table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "217843c7-9e0f-4638-9a09-56422fdb16a9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted successfully.\n"
     ]
    }
   ],
   "source": [
    "#path to the text file \n",
    "file_path = \"lse.txt\"\n",
    "\n",
    "#SQL query to insert data into the table\n",
    "insert_query = \"\"\"INSERT INTO ticker_and_name (Symbol, description) VALUES (%s, %s);\"\"\"\n",
    "\n",
    "#connection to the database and creates a cursor\n",
    "connection = connect_to_database_AWS_postgreSQL()\n",
    "cursor = connection.cursor()\n",
    "\n",
    "try:\n",
    "    # Opens the file for reading\n",
    "    with open(file_path, \"r\") as file:\n",
    "        # Reads each line in the file\n",
    "        for line in file:\n",
    "            \n",
    "            # Splits each line into columns based on a delimiter (e.g., tab)\n",
    "            parts = line.strip().split('\\t')  # Assumes tab as the delimiter\n",
    "\n",
    "            # Check if the line contains the expected number of columns\n",
    "            if len(parts) == 2:  # Assuming 2 columns (Symbol and Description)\n",
    "                symbol, description = parts\n",
    "\n",
    "                # Executes the INSERT query for each row\n",
    "                cursor.execute(insert_query, (symbol, description))\n",
    "\n",
    "    # Commits the changes to the database\n",
    "    connection.commit()\n",
    "    print(\"Data inserted successfully.\")\n",
    "\n",
    "except (Exception, psycopg2.Error) as error:\n",
    "    print(f\"Error: {error}\")\n",
    "    if connection:\n",
    "        connection.rollback()\n",
    "\n",
    "finally:\n",
    "    # Closes the cursor and connection\n",
    "    if cursor:\n",
    "        cursor.close()\n",
    "    if connection:\n",
    "        connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bd83c3-b6b2-4248-b5f8-2a734c9dfd14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06bf7aa5-71cd-4390-a4be-5bd417e861f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOW THAT SYMBOLS AND DESCRIPTION HAVE BEEN ADDED TO poseidon THE REST CAN CONTINUE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4108d1ed-7c47-49d4-8a0a-603d083964c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSERT STOCK INFORMATION Into poseidon s_historical_stock_data the bwlow code does that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c3c27189-4add-438d-8519-0f57ee8f0115",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fetch_historical_data_AWS_postgreSQL(symbol):\n",
    "    try:\n",
    "        stock = yf.Ticker(symbol)\n",
    "        end_date = datetime.now().strftime('%Y-%m-%d')  # Explicitly calling now() function\n",
    "        hist = stock.history(start=\"2005-01-01\", end=end_date)\n",
    "        if hist.empty:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        hist.reset_index(inplace=True)\n",
    "        hist['Symbol'] = symbol\n",
    "\n",
    "        #the desired column order, \n",
    "        #Because I was receiving lots of columns that were not important, therefore I only want the specified columns.\n",
    "        desired_columns =['Symbol', 'Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits' ]\n",
    "        return hist[desired_columns]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {symbol}: {str(e)}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a45224-9d53-4282-9f21-6838ee62d623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching customer information/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41eaf8c9-c888-45a3-a145-655e9d294e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*******\n",
    "#Creatig function to fetch Information on copanies that are listed based on symbol.\n",
    "#This is slightly different because it populates information on companies.\n",
    "\n",
    "\n",
    "#Reference cell B\n",
    "def fetch_info_data_AWS_postgreSQL(symbol):\n",
    "    try:\n",
    "        stock = yf.Ticker(symbol)\n",
    "        info = stock.info\n",
    "\n",
    "        selected_columns = [\n",
    "            \"address1\", \"city\", \"state\", \"zip\", \"country\", \"phone\", \"website\",\n",
    "            \"industry\", \"sector\", \"sectorKey\", \"symbol\", \"longName\",\n",
    "            \"fullTimeEmployees\", \"profitMargins\", \"earningsGrowth\", \"revenueGrowth\",\n",
    "            \"marketCap\", \"fiftyTwoWeekHigh\", \"fiftyTwoWeekLow\", \"currency\", \"exchange\"\n",
    "        ]\n",
    "\n",
    "        info_data = {column: info.get(column, \"N/A\") for column in selected_columns}\n",
    "        return pd.DataFrame([info_data])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {symbol}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542e33a1-fc33-4007-96b5-7671ac1b9a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to insert the infromation into the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89016b2a-ef7d-4fd1-8baa-2d14ac198282",
   "metadata": {},
   "outputs": [],
   "source": [
    "#***************\n",
    "# The below function inserts company data into source schema inside table s_customer table. This table was created directly inside postgreSQL\n",
    "# This will be commented out to avoid running by error and rewriting the entire table or causing duplication.\n",
    "#SQL REFERENCE B\n",
    "#Reference cell A\n",
    "#***************\n",
    "\n",
    "\n",
    "\n",
    "def insert_customer_data_AWS_postgreSQL(row):\n",
    "    query = \"\"\"\n",
    "    INSERT INTO s_customer \n",
    "    (Address1, City, State, Zip, Country, Phone, Website, Industry, Sector, SectorKey, Symbol, \n",
    "    Customer_Name, Full_Time_Employees, Profit_Margins, Earnings_Growth, Revenue_Growth, Market_Cap, \n",
    "    Fifty_Two_Week_High, Fifty_Two_Week_Low, Currency, Exchange) \n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "    params = (\n",
    "        row['address1'] if row['address1'] != 'N/A' else None,\n",
    "        row['city'] if row['city'] != 'N/A' else None,\n",
    "        row['state'] if row['state'] != 'N/A' else None,\n",
    "        row['zip'] if row['zip'] != 'N/A' else None,\n",
    "        row['country'] if row['country'] != 'N/A' else None,\n",
    "        row['phone'] if row['phone'] != 'N/A' else None,\n",
    "        row['website'] if row['website'] != 'N/A' else None,\n",
    "        row['industry'] if row['industry'] != 'N/A' else None,\n",
    "        row['sector'] if row['sector'] != 'N/A' else None,\n",
    "        row['sectorKey'] if row['sectorKey'] != 'N/A' else None,\n",
    "        row['symbol'] if row['symbol'] != 'N/A' else None,\n",
    "        row['longName'] if row['longName'] != 'N/A' else None,\n",
    "        int(row['fullTimeEmployees']) if row['fullTimeEmployees'] != 'N/A' else None,\n",
    "        float(row['profitMargins']) if row['profitMargins'] != 'N/A' else None,\n",
    "        float(row['earningsGrowth']) if row['earningsGrowth'] != 'N/A' else None,\n",
    "        float(row['revenueGrowth']) if row['revenueGrowth'] != 'N/A' else None,\n",
    "        float(row['marketCap']) if row['marketCap'] != 'N/A' else None,\n",
    "        float(row['fiftyTwoWeekHigh']) if 'fiftyTwoWeekHigh' in row and row['fiftyTwoWeekHigh'] != 'N/A' else None,\n",
    "        float(row['fiftyTwoWeekLow']) if 'fiftyTwoWeekLow' in row and row['fiftyTwoWeekLow'] != 'N/A' else None,\n",
    "        row['currency'] if row['currency'] != 'N/A' else None,\n",
    "        row['exchange'] if row['exchange'] != 'N/A' else None\n",
    "    )\n",
    "    execute_sql_query_AWS_postgreSQL(query, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "20b06f70-67dc-43cc-9f6d-f2feed4a8f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inserting the information data as it is being pulled back into the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "743cb249-d105-464b-b6d1-1a8b8a84b3e7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith open(\"lse.txt\", \"r\", encoding=\\'utf-8\\') as file:\\n    next(file)  # Skip the header line\\n    symbols = [line.split(\\'\\t\\')[0].strip() for line in file]\\n\\nbatch_size = 100\\nerror_symbols = []\\n\\nfor i in range(0, len(symbols), batch_size):\\n    batch_symbols = symbols[i:i + batch_size]\\n    for symbol in batch_symbols:\\n        info_df = fetch_info_data_AWS_postgreSQL(symbol)\\n        if info_df is not None and not info_df.empty:\\n            for _, row in info_df.iterrows():\\n                insert_customer_data_AWS_postgreSQL(row)\\n        else:\\n            error_symbols.append(symbol)  # Append symbol to error list if data fetch fails\\n\\n    print(f\"Processed batch {i // batch_size + 1}/{len(symbols) // batch_size + 1}\")\\n\\n\\nprint(\"All data processed and inserted into source.s_customer\")\\nif error_symbols:\\n    with open(\\'s_customer_error_symbols.csv\\', \\'w\\') as f:\\n        for symbol in error_symbols:\\n            f.write(f\"{symbol}\\n\")\\n    print(f\"Symbols with errors written to \\'s_customer_error_symbols.csv\\'\")\\n\\n'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#*****************\n",
    "#This code uses both function above and loops through symbols in bathc of 100 and fetches their information \n",
    "#Then once information is fetched it is then inserted into the the respective table\n",
    "#This code will be commented out. To avoid running by error and rewriting the entire table or causing duplication.\n",
    "#SQL REFERENCE B\n",
    "#Reference cell B\n",
    "#*****************\n",
    "\n",
    "'''\n",
    "with open(\"lse.txt\", \"r\", encoding='utf-8') as file:\n",
    "    next(file)  # Skip the header line\n",
    "    symbols = [line.split('\\t')[0].strip() for line in file]\n",
    "\n",
    "batch_size = 100\n",
    "error_symbols = []\n",
    "\n",
    "for i in range(0, len(symbols), batch_size):\n",
    "    batch_symbols = symbols[i:i + batch_size]\n",
    "    for symbol in batch_symbols:\n",
    "        info_df = fetch_info_data_AWS_postgreSQL(symbol)\n",
    "        if info_df is not None and not info_df.empty:\n",
    "            for _, row in info_df.iterrows():\n",
    "                insert_customer_data_AWS_postgreSQL(row)\n",
    "        else:\n",
    "            error_symbols.append(symbol)  # Append symbol to error list if data fetch fails\n",
    "\n",
    "    print(f\"Processed batch {i // batch_size + 1}/{len(symbols) // batch_size + 1}\")\n",
    "\n",
    "\n",
    "print(\"All data processed and inserted into source.s_customer\")\n",
    "if error_symbols:\n",
    "    with open('s_customer_error_symbols.csv', 'w') as f:\n",
    "        for symbol in error_symbols:\n",
    "            f.write(f\"{symbol}\\n\")\n",
    "    print(f\"Symbols with errors written to 's_customer_error_symbols.csv'\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8560a9e9-4778-4b23-9c43-938a25fad803",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02NG.L: Period '1mo' is invalid, must be one of ['1d', '5d']\n",
      "0DK9.L: No timezone found, symbol may be delisted\n",
      "0DU3.L: No timezone found, symbol may be delisted\n",
      "0E12.L: No timezone found, symbol may be delisted\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 46\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m records_to_insert:\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# Handle NaN values and convert them to None\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     modified_record \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misna(value) \u001b[38;5;28;01melse\u001b[39;00m value \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m record])\n\u001b[1;32m---> 46\u001b[0m     execute_sql_query_AWS_postgreSQL(insert_query, modified_record)\n\u001b[0;32m     47\u001b[0m     record_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mbatch_size\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Inserted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecord_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m records.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[25], line 8\u001b[0m, in \u001b[0;36mexecute_sql_query_AWS_postgreSQL\u001b[1;34m(query, params)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_sql_query_AWS_postgreSQL\u001b[39m(query, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m      6\u001b[0m     \n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Establish a connection to the database\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     connection \u001b[38;5;241m=\u001b[39m connect_to_database_AWS_postgreSQL()\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m connection:\n",
      "Cell \u001b[1;32mIn[47], line 17\u001b[0m, in \u001b[0;36mconnect_to_database_AWS_postgreSQL\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     database \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatabase\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatabase\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     16\u001b[0m    \u001b[38;5;66;03m# Establish a connection to the database using the credentials from the config file\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     connection \u001b[38;5;241m=\u001b[39m psycopg2\u001b[38;5;241m.\u001b[39mconnect(host\u001b[38;5;241m=\u001b[39mhost, port\u001b[38;5;241m=\u001b[39mport, user\u001b[38;5;241m=\u001b[39muser, password\u001b[38;5;241m=\u001b[39mpassword, database\u001b[38;5;241m=\u001b[39mdatabase)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, psycopg2\u001b[38;5;241m.\u001b[39mError) \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\psycopg2\\__init__.py:122\u001b[0m, in \u001b[0;36mconnect\u001b[1;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     kwasync[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124masync_\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124masync_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    121\u001b[0m dsn \u001b[38;5;241m=\u001b[39m _ext\u001b[38;5;241m.\u001b[39mmake_dsn(dsn, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 122\u001b[0m conn \u001b[38;5;241m=\u001b[39m _connect(dsn, connection_factory\u001b[38;5;241m=\u001b[39mconnection_factory, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwasync)\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cursor_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     conn\u001b[38;5;241m.\u001b[39mcursor_factory \u001b[38;5;241m=\u001b[39m cursor_factory\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Path to file containing symbols\n",
    "file_path = \"lse.txt\"\n",
    "symbols = []\n",
    "\n",
    "# Flag to start adding symbols\n",
    "start_adding = True\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    next(file)  # Skip the header\n",
    "    for line in file:\n",
    "        symbol = line.split()[0].strip()\n",
    "        # Skip invalid symbols\n",
    "        if symbol in ('null', '0.0', ''):\n",
    "            continue\n",
    "        if start_adding:\n",
    "            symbols.append(symbol)\n",
    "\n",
    "\n",
    "#SQL INSERT statement here\n",
    "insert_query = \"\"\"\n",
    "    INSERT INTO s_historical_stock_data (symbol, date, open, high, low, close, volume, dividends, stock_splits)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "# Process symbols in batches of 100\n",
    "batch_size = 100\n",
    "for i in range(0, len(symbols), batch_size):\n",
    "    batch_symbols = symbols[i:i + batch_size]\n",
    "    all_historical_data = pd.DataFrame()\n",
    "\n",
    "    for symbol in batch_symbols:\n",
    "        historical_data = fetch_historical_data_AWS_postgreSQL(symbol)\n",
    "        if not historical_data.empty:\n",
    "            all_historical_data = pd.concat([all_historical_data, historical_data])\n",
    "\n",
    "    if not all_historical_data.empty:\n",
    "        records_to_insert = all_historical_data.to_records(index=False).tolist()\n",
    "\n",
    "        record_count = 0\n",
    "        for record in records_to_insert:\n",
    "            # Handle NaN values and convert them to None\n",
    "            modified_record = tuple([None if pd.isna(value) else value for value in record])\n",
    "\n",
    "            execute_sql_query_AWS_postgreSQL(insert_query, modified_record)\n",
    "            record_count += 1\n",
    "\n",
    "        print(f\"Batch {i//batch_size + 1}: Inserted {record_count} records.\")\n",
    "    else:\n",
    "        print(f\"Batch {i//batch_size + 1}: No data to insert.\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410f0b69-50a6-48e8-805c-2ddfafcfc1e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#NEXT STEP IS TO POPULATE S_CUSTOMER DATa ON AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb5af29-011b-4e52-9edd-7c19f45e0274",
   "metadata": {},
   "outputs": [],
   "source": [
    "#appending s_date to d_dated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7377c049-f89a-4b84-8c89-a54436c2cae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_to_append_s_date_to_d_dated=\"\"\"\n",
    "INSERT INTO d_dated (Date, Year, Quarter, Month, Week, Day, DayOfWeek, IsWeekend, IsHoliday, FiscalYear, FiscalQuarter, FiscalMonth)\n",
    "SELECT Date, Year, Quarter, Month, Week, Day, DayOfWeek, IsWeekend, IsHoliday, FiscalYear, FiscalQuarter, FiscalMonth \n",
    "FROM s_date;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259fadf8-430a-4026-a20c-d6a98873ac05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append Missing customer details using case statment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae6e4093-d2a9-4ab2-8cd7-4a1d316c2ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rectify_missing_customer_names=\"\"\" UPDATE s_customer AS x\n",
    "SET customer_name = CASE\n",
    "    WHEN x.customer_name IS NULL THEN y.description\n",
    "    ELSE x.customer_name\n",
    "END\n",
    "FROM ticker_and_name AS y\n",
    "WHERE x.symbol = y.symbol;\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e174bec-4b8a-4a12-a263-f1e469999963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#execute_sql_query_AWS_postgreSQL(rectify_missing_customer_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c02daba5-1345-4d68-b7bc-1876309124e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#execute_sql_query_AWS_postgreSQL(query_to_append_s_date_to_d_dated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d803fbcd-45c5-4dd8-95fe-05574725dab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appending corporate inforamtion to d_customer. \n",
    "#since this has been aleady achived, This will be commented out \n",
    "\n",
    "'''\n",
    "Querr_to_append_corporate_info_to_d_customer=\"\"\" INSERT INTO d_customer (\n",
    "    Address,\n",
    "    City,\n",
    "    State,\n",
    "    Zip,\n",
    "    Country,\n",
    "    Phone,\n",
    "    Website,\n",
    "    Industry,\n",
    "    Sector,\n",
    "    SectorKey,\n",
    "    symbol,\n",
    "    Company_name,\n",
    "    full_time_employees,\n",
    "    Profit_margins,\n",
    "    Earnings_growth,\n",
    "    Revenue_growth,\n",
    "    Market_cap,\n",
    "    Fifty_two_week_high,\n",
    "    Fifty_two_week_low,\n",
    "    Currency,\n",
    "    Exchange\n",
    ")\n",
    "SELECT\n",
    "    Address1,\n",
    "    City,\n",
    "    State,\n",
    "    Zip,\n",
    "    Country,\n",
    "    Phone ,\n",
    "    Website ,\n",
    "    Industry ,\n",
    "    Sector ,\n",
    "    SectorKey ,\n",
    "\tsymbol,\n",
    "    customer_name,\n",
    "    full_time_employees,\n",
    "    profit_margins,\n",
    "    earnings_growth ,\n",
    "    revenue_growth ,\n",
    "    market_cap ,\n",
    "    fifty_two_week_high ,\n",
    "    fifty_two_week_low ,\n",
    "    currency,\n",
    "    exchange \n",
    "FROM\n",
    "    s_customer \"\"\"\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8da84051-0b10-429a-811b-03a5606d1e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#execute_sql_query_AWS_postgreSQL(Querr_to_append_corporate_info_to_d_customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7202831b-612e-4b73-8c4e-afcd2dea81bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next step is to ensure only uniqe records of stock information is added to di_financial_info\n",
    "#Thus when the initial download finiashe this will be implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "83cfd89c-1ab0-420b-ad76-19795a40e2a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "933bacba-9c02-4cc7-97b2-c78209791597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cfb2c4fc-2862-4d4e-bc17-2db91b4b042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inserting_unique_records_into_di_financial_info=\"\"\"INSERT INTO di_financial_info (\n",
    "    symbol, \n",
    "    date,\n",
    "    open,\n",
    "    high,\n",
    "    low,\n",
    "    close,\n",
    "    volume,\n",
    "    dividends,\n",
    "    stock_splits\n",
    ")\n",
    "SELECT \n",
    "    symbol, \n",
    "    date,\n",
    "    open,\n",
    "    high,\n",
    "    low,\n",
    "    close,\n",
    "    volume,\n",
    "    dividends,\n",
    "    stock_splits\n",
    "FROM \n",
    "    s_historical_stock_data AS s\n",
    "WHERE NOT EXISTS (\n",
    "    SELECT 1 \n",
    "    FROM di_financial_info AS d \n",
    "    WHERE \n",
    "        d.symbol = s.symbol \n",
    "        AND d.date = s.date\n",
    ");\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1f209c16-547f-4946-a99a-a4c07d61ce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_sql_query_AWS_postgreSQL(inserting_unique_records_into_di_financial_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7c3976dd-2aea-4886-afdd-7b390cf67a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The next step is to append this information to facts. \n",
    "#The table consist of three dimensions. The next step will be to join them up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a7d96e6-7885-42f1-8c8a-8d38c8f5b31a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "populate_facts_table= \"\"\"truncate predictive_model6_results_aws_postgresql\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7de9a80e-ad2d-40f6-bb75-e3c725764114",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "execute_sql_query_AWS_postgreSQL(populate_facts_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "64242625-5f13-4a19-8e80-6f3720063930",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test=\"\"\"Select count(symbol) from predictive_model5_results_aws_postgresql\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8abc1002-bb8e-40da-81e4-1b28496359fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(39376,)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execute_sql_query_AWS_postgreSQL(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c389a91-c0a0-4f59-a97b-ef83b58aec73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
